{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## [Solution]\n",
    "Same number of training data labeled 1 and 0: can use an uninformative uniform prior 1/k=1/2. So we assume $p()$ Since the feature vectors basically consist of boolean values a Bernoulli distribution is probably a suitable choice for the likelihood. \n",
    "\n",
    "Fit distributions to sample data. MLE is the sample mean. So I guess $\\mu_1 = (\\mu_{11}, \\mu_{12}, \\mu_{13}) = (3/4, 2/4, 3/4)$ and $\\mu_2 = (\\mu_{21}, \\mu_{22}, \\mu_{23}) = (1/4, 1/4, 1/4)$. Ah since we're doing naive bayes we just have a product of three bernoulli dists instead of vector valued means. \n",
    "\n",
    "Could use a prior where we account for the label -> feature frequency, i.e. \"how many happy people are married\".\n",
    "\n",
    "Since we're using a naive-bayes classifier, we assume that the elements of each feature vector are independed. This means that we can write the likelihood as\n",
    "\n",
    "$p(x_{new}| t_{new} = k, X, t) = p(x_{new 1}| t_{new} = k, X, t) p(x_{new 2}| t_{new} = k, X, t) p(x_{new 3}| t_{new} = k, X, t)$\n",
    "\n",
    "So using $\\mu_{1,2}$ from above and the probability mass function of the Bernoulli distribution we get:\n",
    "\n",
    "$p(x_{new}| t_{new} = k, X, t) = \\mu_{k1}^{x_1}(1-\\mu_{k1})^{(1-x_1)} \\cdot \\mu_{k2}^{x_2}(1-\\mu_{k2})^{(1-x_2)} \\cdot \\mu_{k3}^{x_3}(1-\\mu_{k3})^{(1-x_3)}$\n",
    "\n",
    "So for $k=1$ we get:\n",
    "$p(x_{new}| t_{new} = 1, X, t) = \\mu_{11}^{x_1}(1-\\mu_{11})^{(1-x_1)} \\cdot \\mu_{12}^{x_2}(1-\\mu_{12})^{(1-x_2)} \\cdot \\mu_{13}^{x_3}(1-\\mu_{13})^{(1-x_3)}$\n",
    "\n",
    "With values from $\\mu_1$:\n",
    "\n",
    "$p(x_{new}| t_{new} = 1, X, t) = (3/4)^{x_1}(1-(3/4))^{(1-x_1)} \\cdot (2/4)^{x_2}(1-(2/4))^{(1-x_2)} \\cdot (3/4)^{x_3}(1-(3/4))^{(1-x_3)}$\n",
    "\n",
    "Analagous steps for $k=2$:\n",
    "\n",
    "$p(x_{new}| t_{new} = 2, X, t) = \\mu_{21}^{x_1}(1-\\mu_{21})^{(1-x_1)} \\cdot \\mu_{22}^{x_2}(1-\\mu_{22})^{(1-x_2)} \\cdot \\mu_{23}^{x_3}(1-\\mu_{23})^{(1-x_3)}$\n",
    "\n",
    "Plugging in values from $\\mu_2$:\n",
    "\n",
    "$p(x_{new}| t_{new} = 2, X, t) = (1/4)^{x_1}(1-(1/4))^{(1-x_1)} \\cdot (1/4)^{x_2}(1-(1/4))^{(1-x_2)} \\cdot (1/4)^{x_3}(1-(1/4))^{(1-x_3)}$\n",
    "\n",
    "Since we have already assumed the prior distributions $p(t_{new}=1) = p(t_{new}=2) = 1/4$ we now have all the parts needed to apply Bayes rule:\n",
    "\n",
    "$p(t_{new} = 1 | x_{new}, X, t) = \\frac{p(x_{new}| t_{new} = 1, X, t) p(t_{new}=1)}{\\sum_j p(x_{new}| t_{new} = j, X, t) p(t_{new}=j)}$\n",
    "\n",
    "For simplicity, we precompute the likelihoods:\n",
    "### [Test case 1]\n",
    "$$ p(x_{new}| t_{new} = 1, X, t) =  (3/4)^{x_1}(1-(3/4))^{(1-x_1)} \\cdot (2/4)^{x_2}(1-(2/4))^{(1-x_2)} \\cdot (3/4)^{x_3}(1-(3/4))^{(1-x_3)}$$\n",
    "$$ p(x_{new}| t_{new} = 2, X, t) = (1/4)^{x_1}(1-(1/4))^{(1-x_1)} \\cdot (1/4)^{x_2}(1-(1/4))^{(1-x_2)} \\cdot (1/4)^{x_3}(1-(1/4))^{(1-x_3)} $$\n",
    "\n",
    "Plugging in the values from $x_{new}$:\n",
    "\n",
    "$$ p(x_{new}| t_{new} = 1, X, t) =  (3/4)^{0}(1-(3/4))^{(1-0)} \\cdot (2/4)^{1}(1-(2/4))^{(1-1)} \\cdot (3/4)^{1}(1-(3/4))^{(1-1)} = (1-(3/4))\\cdot (2/4)\\cdot (3/4) = 3/32$$\n",
    "$$ p(x_{new}| t_{new} = 2, X, t) = (1/4)^{0}(1-(1/4))^{(1-0)} \\cdot (1/4)^{1}(1-(1/4))^{(1-1)} \\cdot (1/4)^{1}(1-(1/4))^{(1-1)} = (1-(1/4)) \\cdot (1/4) \\cdot (1/4) = 3/64$$\n",
    "\n",
    "In conclusion, we get:\n",
    "\n",
    "$ p(t_{new} = 1 | x_{new}, X, t) = \\frac{3/32 \\cdot 1/4}{3/32 \\cdot 1/4 + 3/64\\cdot1/4} = \\frac{3/128}{3/128 + 3/256} = \\frac{3/128}{3/128} \\frac{1}{1+1/2} = 2/3$\n",
    "\n",
    "So based on this small data set and using naive bayes, a person that is not rich but married and healthy has about a $.66$ probability of being content.\n",
    "\n",
    "### [Test case 2]\n",
    "When one or more features are missing from a test feature vector, we can marginalize over that feature. This means we must consider all possibilities for the missing feature, i.e. both healthy and not healthy:\n",
    "\n",
    "$p(x_{new}| t_{new} = k, X, t) = \\sum_{x_3 = 0}^1\\mu_{k1}^{x_1}(1-\\mu_{k1})^{(1-x_1)} \\cdot \\mu_{k2}^{x_2}(1-\\mu_{k2})^{(1-x_2)} \\cdot \\mu_{k3}^{x_3}(1-\\mu_{k3})^{(1-x_3)}$\n",
    "$p(x_{new}| t_{new} = k, X, t) = (\\mu_{k3} + (1-\\mu_{k3})) \\mu_{k1}^{x_1}(1-\\mu_{k1})^{(1-x_1)} \\cdot \\mu_{k2}^{x_2}(1-\\mu_{k2})^{(1-x_2)}$\n",
    "$p(x_{new}| t_{new} = k, X, t) = (1 + \\mu_{k3} -\\mu_{k3})) \\mu_{k1}^{x_1}(1-\\mu_{k1})^{(1-x_1)} \\cdot \\mu_{k2}^{x_2}(1-\\mu_{k2})^{(1-x_2)}$\n",
    "$p(x_{new}| t_{new} = k, X, t) = \\mu_{k1}^{x_1}(1-\\mu_{k1})^{(1-x_1)} \\cdot \\mu_{k2}^{x_2}(1-\\mu_{k2})^{(1-x_2)}$\n",
    "\n",
    "Using this and plugging in $\\mu_{1,2}$ like before we get these likelihoods for the two classes:\n",
    "With values from $\\mu_1$:\n",
    "\n",
    "$p(x_{new}| t_{new} = 1, X, t) = (3/4)^{x_1}(1-(3/4))^{(1-x_1)} \\cdot (2/4)^{x_2}(1-(2/4))^{(1-x_2)}$\n",
    "\n",
    "$p(x_{new}| t_{new} = 2, X, t) = (1/4)^{x_1}(1-(1/4))^{(1-x_1)} \\cdot (1/4)^{x_2}(1-(1/4))^{(1-x_2)}$\n",
    "\n",
    "Plugging in the values from $x_{new}$:\n",
    "\n",
    "$$ p(x_{new}| t_{new} = 1, X, t) =  (3/4)^{0}(1-(3/4))^{(1-0)} \\cdot (2/4)^{1}(1-(2/4))^{(1-1)} = (1-(3/4))\\cdot (2/4) = 1/8$$\n",
    "$$ p(x_{new}| t_{new} = 2, X, t) = (1/4)^{0}(1-(1/4))^{(1-0)} \\cdot (1/4)^{1}(1-(1/4))^{(1-1)}  = (1-(1/4)) \\cdot (1/4) = 3/16$$\n",
    "\n",
    "Like before we use the priors $p(t_{new}=1) = p(t_{new}=2) = 1/4$ and arrive at:\n",
    "\n",
    "$ p(t_{new} = 1 | x_{new}, X, t) = \\frac{1/8 \\cdot 1/4}{1/8 \\cdot 1/4 + 3/16 \\cdot 1/4} = \\frac{1/8 }{1/8  + 3/16} = \\frac{1}{1 + 3/2} = 2/(2+3) = 2/5$\n",
    "\n",
    "So if we don't know if the person is healthy or not, the probability of them being content drops to 2/5, or 0.4.\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n",
    "\n",
    "## [Solution]\n",
    "\n",
    "The big problem here is that the variables $x_1, x_2, x_3$ aren't indpendent. Only one of them can be true for a given observation. This breaks the independence assumption in Naive Bayes. A possible solution is to combine these three binary features to one multi-value feature. Our new vector of attributes looks like this:\n",
    "\n",
    "* $x_1 = 0$ if customer is younger than 20, 1 between 20 and 30 in age and 2 otherwise.\n",
    "* $x_2 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Solution]\n",
    "a.\n",
    "In general we have:\n",
    "\n",
    "$P(y_{new} = k|X, y, x_{new}) = \\frac{P(x_{new}|y_{new} = k, X, y) P(y_{new} = k)}{\\Sigma_j P(x_{new}|y_{new} = j, X, y)P(y_{new} = j)}$\n",
    "\n",
    "In our case (with the hints) we get:\n",
    "\n",
    "$P(y_{new} = 1) = P(y_{new} = -1) = \\frac{1}{2}$\n",
    "\n",
    "$\\Sigma_j P(x_{new}|y_{new} = j, X, y)P(y_{new} = j) = \\frac{1}{2}(P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1}) + P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})$\n",
    "\n",
    "Finally, plugging that into the general expression we get:\n",
    "\n",
    "$P(y_{new} = +1 | x_{new} , X, y ) = \\frac{P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})}{P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1}) + P(x_{new} |\n",
    "\\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})}$\n",
    "\n",
    "$P(y_{new} = -1 | x_{new} , X, y ) = \\frac{P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})}{P(x_{new} | \\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1}) + P(x_{new} | \\hat{\\mu}_{2}, \\hat{\\sigma}^{2}_{2})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "data = np.genfromtxt('dataset2.txt', delimiter=',')\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2])\n",
    "plt.show()\n",
    "LABEL1 = 1\n",
    "LABEL2 = -1\n",
    "\n",
    "def sge(X):\n",
    "    \"\"\"\n",
    "    SGE Mean and variance estimator for spherical Gaussian distribution\n",
    "\n",
    "    X : Data matrix of size n x p where each row represents a p-dimensional data point\n",
    "    e.g. X = [ 2 1 ; 3 7 ; 4 5 ] is a dataset having 3 samples having two co−ordinates each.\n",
    "    mu : Estimated mean o f the dataset [mu_1 mu_2 . . . mu_p]\n",
    "    variance : Estimated variance of the dataset ( number )\n",
    "    \"\"\"\n",
    "    n=X.shape[0]\n",
    "    p=X.shape[1]\n",
    "    mu = np.mean(X, axis=0)\n",
    "    diffs = X - mu\n",
    "    variance  = 0\n",
    "    for diff in diffs:\n",
    "        variance += 1/(n*p) * diff.dot(diff.T)\n",
    "    return mu, variance\n",
    "\n",
    "\n",
    "def sph_bayes(Xtest, mu1, sigma1, mu2, sigma2):\n",
    "    I = np.identity(len(mu1))\n",
    "    p1 = multivariate_normal(mu1, I*sigma1).pdf(Xtest)\n",
    "    p2 = multivariate_normal(mu2, I*sigma2).pdf(Xtest)\n",
    "    P1 = p1/(p1+p2)\n",
    "    P2 = p2/(p1+p2)\n",
    "    Ytest = LABEL1 if P1 > P2 else LABEL2\n",
    "    return [P1, P2, Ytest]\n",
    "\n",
    "def new_classifier(Xtest, mu1, mu2):\n",
    "    b = 0.5*(mu1 + mu2)\n",
    "    norm = np.linalg.norm(mu1-mu2)\n",
    "    Ytest = np.sign((mu1 - mu2).T.dot(Xtest - b)/norm)\n",
    "    return [Ytest]\n",
    "\n",
    "def stupid_classifier(Xtest):\n",
    "    return LABEL1 if np.random.rand() < 0.5 else LABEL2\n",
    "\n",
    "# Split data in 5 sets and do cross validation\n",
    "avg_bayes_error = 0\n",
    "avg_new_class_error = 0\n",
    "num_splits = 5\n",
    "split_size = int(X.shape[0] / num_splits)\n",
    "for k in range(num_splits):\n",
    "    X_train = np.concatenate((X[:k * split_size], X[(k + 1) * split_size:]), axis=0)\n",
    "    X_test = X[k * split_size:(k + 1) * split_size]\n",
    "    y_train = np.concatenate((y[:k * split_size], y[(k + 1) * split_size:]), axis=0)\n",
    "    y_test = y[k * split_size:(k + 1) * split_size]\n",
    "    \n",
    "    X_train_1 = X_train[y_train==1]\n",
    "    X_train_2 = X_train[y_train==-1]\n",
    "\n",
    "    mu1, sigma1 = sge(X_train_1)\n",
    "    mu2, sigma2 = sge(X_train_2)\n",
    "\n",
    "    bayes_error, new_class_error = 0, 0\n",
    "    stupid_error = 0\n",
    "    for (i,x_test) in enumerate(X_test):\n",
    "        P1, P2, bayes_pred = sph_bayes(x_test, mu1, sigma1, mu2, sigma2)\n",
    "        bayes_error += (bayes_pred != y_test[i])/len(y_test)\n",
    "    \n",
    "        new_class_pred = new_classifier(x_test, mu1, mu2)\n",
    "        new_class_error += (new_class_pred != y_test[i])/len(y_test)\n",
    "        \n",
    "        stupid_pred = stupid_classifier(x_test)\n",
    "        stupid_error += np.sum(stupid_pred != y_test[i])/len(y_test)\n",
    "                \n",
    "    avg_bayes_error += bayes_error / num_splits\n",
    "    avg_new_class_error += new_class_error / num_splits\n",
    "\n",
    "    print('Bayes spherical error: %.5f' % bayes_error)\n",
    "    print('New classifier error: %.5f' % new_class_error)\n",
    "    print('Stupid error: %.5f' % stupid_error)\n",
    "    \n",
    "print('Average bayes spherical error: %.5f' % avg_bayes_error)\n",
    "print('Average new classifier error: %.5f' % avg_new_class_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
