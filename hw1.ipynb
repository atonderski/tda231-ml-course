{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 1** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Maximum likelihood estimation (MLE), Maximum a posteriori (MAP)**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Vasileios** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 16/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: ** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$$\\qquad$ Adam Tonderski, 930524-1037, tadam@student.chalmers.se <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$$\\qquad$ Britta Thörnblom, 900718-1341, brittat@student.chalmers.se <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Maximum likelihood estimator (MLE), 4 points]\n",
    "\n",
    "Consider a dataset $x_1, \\ldots, x_n$ consisting of i.i.d. observations \n",
    "generated from a **spherical** multivariate Gaussian distribution $N(\\mu, \\sigma^2I)$, where $\\mu \\in \\mathbb{R}^p$, $I$ \n",
    "is the $p \\times p$ identity matrix, and $\\sigma^2$ is a\n",
    "scalar. Derive the maximum likelihood estimator for $\\sigma$.\n",
    "\n",
    "### [Solution]\n",
    "The multivariate Gaussian distribution $N(\\mu, \\sigma^2I)$ of dimension $p$ has pdf:\n",
    "\n",
    "$$ f(\\vec{x}) = \\frac{1}{\\sqrt{(2\\pi)^p \\vert \\sigma^2 I \\vert}} \\text{exp} \\left(-1/2 (\\vec{x} - \\mu)(\\sigma^2 I)^{-1}(\\vec{x} - \\mu) \\right) $$\n",
    "\n",
    "The joint log likelihood is given by:\n",
    "\n",
    "$\\ln L = \\ln \\Pi_{i=0}^n f(\\vec{x}_i) = \\Sigma_{i=0}^n \\ln f(\\vec{x}_i)$\n",
    "\n",
    "For a single $\\vec{x}_i$ the log likelihood is:\n",
    "\n",
    "$\\ln f(\\vec{x}_i) = \\ln((2\\pi)^{-p/2}) + \\ln(\\vert \\sigma^2 I \\vert^{-1/2}) -1/2 (\\vec{x}_i - \\mu)(\\sigma^2 I)^{-1}(\\vec{x}_i - \\mu)$\n",
    "\n",
    "Using $\\vert \\sigma^2 I \\vert = \\sigma^{2p}$ we can simplify this to:\n",
    "\n",
    "$\\ln f(\\vec{x}_i) = -1/2\\left( p \\ln(2\\pi) + p\\ln(\\sigma^2) + (\\vec{x}_i - \\mu)(\\sigma^2 I)^{-1}(\\vec{x}_i - \\mu)\\right)$\n",
    "\n",
    "Taking the derivative $\\ln L $ with respect to $\\sigma^2$ yields:\n",
    "\n",
    "$ \\frac{\\partial ln(L)}{\\partial \\sigma^2} = \\Sigma_{i=0}^n \\left( \\frac{-p}{2\\sigma^2} + \\frac{1}{(\\sigma^2)^2} (\\vec{x}_i - \\mu)^T(\\vec{x}_i - \\mu) \\right) $\n",
    "\n",
    "Then we solve $ \\frac{\\partial ln(L)}{\\partial \\sigma^2} = 0$ for $\\sigma^2$:\n",
    "\n",
    "$\\Sigma_{i=0}^n \\left( \\frac{1}{(\\sigma^2)^2} (\\vec{x}_i - \\mu)^T(\\vec{x}_i - \\mu) \\right)=\\frac{np}{2\\sigma^2}$\n",
    "\n",
    "$\\frac{1}{\\sigma^2}\\Sigma_{i=0}^n \\left((\\vec{x}_i - \\mu)^T(\\vec{x}_i - \\mu) \\right)=\\frac{np}{2}$\n",
    "\n",
    "$\\sigma^2=\\frac{2}{np} \\Sigma_{i=0}^n \\left((\\vec{x}_i - \\mu)^T(\\vec{x}_i - \\mu) \\right)$\n",
    "\n",
    "The maximum likelihood estimator for $\\sigma^2$ is:\n",
    "\n",
    "$\\hat{\\sigma^2}=\\frac{2}{np} \\Sigma_{i=0}^n \\left((\\vec{x}_i - \\mu)^T(\\vec{x}_i - \\mu) \\right)$\n",
    "\n",
    "\n",
    "## [Posterior distributions, 6 points]\n",
    "\n",
    "Consider dataset $x_1, \\ldots, x_n $ consisting of i.i.d. observations \n",
    "generated from a **spherical** multivariate Gaussian distribution $N(\\mu, \\sigma^2I)$, where $\\mu =\n",
    "[\\mu_{1},\\, \\mu_{2}]^{\\top} \\in \\mathbb{R}^2$, $I$ \n",
    "is the $2 \\times 2$ identity matrix, and $\\sigma^2$ is a scalar. \n",
    "The probability distribution of a point $\\mathbf{x}=[x_{1},\\, x_{2}]^{\\top}$ is given by\n",
    "\n",
    "$$ P(X = x \\,|\\, \\sigma^{2}) =  \\frac{1}{ 2\\pi \\sigma^2}   \\exp\n",
    "\\left( -\\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2\\sigma^{2}} \\right)\n",
    "~.$$\n",
    "\n",
    "We assume that $\\sigma^{2}$ has an **inverse-gamma** prior distribution\n",
    "given by\n",
    "$$ P(\\sigma^{2} = s | \\alpha, \\beta) =\n",
    "\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} s^{-\\alpha-1} \\exp\\left( -\n",
    "  \\frac{\\beta}{s}\\right)~. \\tag{1} $$\n",
    "  \n",
    "where $\\alpha$ and $\\beta$ are parameters and $\\Gamma(\\cdot)$ is the\n",
    "gamma function given by $\\Gamma(x) = \\int_{0}^{\\infty} t^{x-1} e^{-t}\n",
    "dt $.\n",
    "\n",
    "1. Derive the posterior distribution $p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha, \\beta)$. (HINT: inverse-gamma distribution is conjugate prior to sphericalGaussian distribution when mean is known).\n",
    "\n",
    "2. Assume $\\mu$ is known and consider two separate models (having different parameters)\n",
    "\n",
    "    * $\\alpha =1$ and $\\beta=1$ (Model $M_{A}$)\n",
    "    * $\\alpha = 10$ and $\\beta= 1$ (Model $M_{B}$) \n",
    "\n",
    "Compute analytically the expression for the MAP estimate for both models in terms of posterior parameters referred to as $\\alpha_{1}, \\beta_{1}$.\n",
    "\n",
    "### [Solution]\n",
    "### [Finding the posterior]\n",
    "Given the hint, we know that the posterior will be of form:\n",
    "\n",
    "$ p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha_1, \\beta_1) = \\frac{\\beta_1^{\\alpha_1}}{\\Gamma (\\alpha_1)} s^{-\\alpha_1-1} \\text{exp}\\left( \\frac{-\\beta_1}{s}\\right) \\tag{2}$\n",
    "\n",
    "We also know that \n",
    "\n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha_1, \\beta_1) \\propto P(X = x \\,|\\, \\sigma^{2}) \\dot P(\\sigma^{2} = s | \\alpha, \\beta)$\n",
    "\n",
    "Using this, we multiply the likelihood $P(X = x \\,|\\, \\sigma^{2})$ by the prior $P(\\sigma^{2} = s | \\alpha, \\beta)$ and identify the parameters $a, b$ of $P(s \\, |\\, a, b)$:\n",
    "\n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha_1, \\beta_1) \\propto \\frac{1}{ 2\\pi s}   \\exp\n",
    "\\left( -\\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2s} \\right) \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} s^{-\\alpha-1} \\exp\\left( -\\frac{\\beta}{s}\\right)~. $\n",
    " \n",
    "Collecting non-exponential factors and multiplying the exponentials together:\n",
    " \n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha_1, \\beta_1) \\propto \\frac{1}{ 2\\pi s} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} s^{-\\alpha-1} \\exp\n",
    "\\left( -\\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2s}  - \\frac{\\beta}{s}\\right)~. $\n",
    "\n",
    "The exponential of $s$ goes from $-\\alpha-1$ to $-\\alpha-2$ (from the denominator), and the factors of $1/s$ in the exponential are lumped together:\n",
    "\n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha_1, \\beta_1) \\propto \\frac{1}{ 2\\pi} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} s^{-\\alpha-2} \\exp\n",
    "\\left( -\\frac{1}{s} \\left( \\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2}  + \\beta\\right)  \\right)~. $\n",
    "\n",
    "Everything occuring to the left of the first $s$ is just a scaling factor and can be safely removed since the normalizing factors will be determined by the final parameters. (Or in other words, we only care about the stuff that affects $s$).\n",
    "\n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha_1, \\beta_1) \\propto s^{-\\alpha-2} \\exp \\left( -\\frac{1}{s} \\left( \\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2}  + \\beta\\right)  \\right)~. $\n",
    "\n",
    "Now we use the inverse-gamma distribution set up from equation 2 and identify $\\alpha_1$ and $\\beta_1$:\n",
    " \n",
    "$\\frac{\\beta_1^{\\alpha_1}}{\\Gamma (\\alpha_1)} s^{-\\alpha_1-1} \\text{exp}\\left( \\frac{-\\beta_1}{s}\\right) \\propto s^{-\\alpha-2} \\exp \\left( -\\frac{1}{s} \\left( \\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2}  + \\beta\\right)  \\right)~.$\n",
    "\n",
    "Ocular inspection yields:\n",
    "\n",
    "$\\alpha_1 = \\alpha  + 1$\n",
    "\n",
    "$\\beta_1 = \\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2}$\n",
    "\n",
    "Including scaling factors this gives us a posterior distribution:\n",
    "\n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha, \\beta) = \\frac{\\left(\\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2}\\right)^{\\alpha  + 1}}{\\Gamma (\\alpha  + 1)} (\\alpha  + 1) ^{-\\alpha - 2} \\text{exp}\\left( \\frac{-\\left(\\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2}\\right)}{s}\\right)$\n",
    "\n",
    "Cleaning this up a bit gives us:\n",
    "\n",
    "$p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha, \\beta) =\\left(\\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2}\\right)^{\\alpha  + 1} \\frac{(\\alpha  + 1) ^{-\\alpha - 2}}{\\Gamma (\\alpha  + 1)}  \\text{exp}\\left( -\\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2s}\\right)$\n",
    "\n",
    "#### [Getting the MAP estimates]\n",
    "\n",
    "The MAP estimate is equal to the mode of the posterior distribution. So now we are lucky for two reasons; we have a closed form of the posterior distribution $p(\\sigma^{2} = s | x_{1} , \\ldots, x_{n}; \\alpha, \\beta)$ and it is of inverse-gamma form. The mode of an inverse-gamma distribution with parameters $\\alpha_1, \\beta_1$ has a simple closed form: $\\frac{\\beta_1}{\\alpha_1+1}$.\n",
    "\n",
    "##### [M_A]\n",
    "With $\\alpha =1$ and $\\beta=1$ we get posterior parameter values:\n",
    "\n",
    "$\\alpha_1 = \\alpha  + 1 = 2$\n",
    "\n",
    "$\\beta_1  = \\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2} = \\frac{(x-\\mu)^T(x-\\mu) + 1}{2}$ \n",
    "    \n",
    "We plug this into the definition of the inverse-gamma mode and get:\n",
    "\n",
    "$\\hat{\\sigma}_\\text{MAP}(x, \\mu) = \\frac{\\beta_1}{\\alpha_1+1} = \\frac{(x-\\mu)^T(x-\\mu) + 1}{2\\cdot (2+1)} = \\frac{(x-\\mu)^T(x-\\mu) + 1}{6}$\n",
    "\n",
    "##### [M_B]\n",
    "Same procedure as above, but now we have parameter values $\\alpha = 10$ and $\\beta= 1$:\n",
    "\n",
    "$\\alpha_1 = \\alpha  + 1 = 11$\n",
    "\n",
    "$\\beta_1  = \\frac{(x-\\mu)^T(x-\\mu) + \\beta}{2} = \\frac{(x-\\mu)^T(x-\\mu) + 1}{2}$ \n",
    "\n",
    "Again we plug this into the mode definition and get:\n",
    "\n",
    "$\\hat{\\sigma}_\\text{MAP}(x, \\mu) = \\frac{\\beta_1}{\\alpha_1+1} = \\frac{(x-\\mu)^T(x-\\mu) + 1}{2\\cdot12} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "**Useful python libraries/functions:**\n",
    "* **General:**  shape, reshape, np.mean etc.\n",
    "* **Plotting:** plot, scatter, legend, hold, imshow, subplot,\n",
    "  grid, title etc.\n",
    "\n",
    "## [Spherical Gaussian estimation, 5 points]\n",
    "\n",
    "Consider a dataset consisting of i.i.d. observations\n",
    "generated from a spherical Gaussian distribution $N(\\mu, \\sigma^2I)$, where $\\mu \\in \\mathbb{R}^p$, $I$ \n",
    "is the $p \\times p $ identity matrix, and $\\sigma^2$ is a scalar.\n",
    "\n",
    "(a) Write the mathematical expression for the MLE estimators for $\\mu$ and $\\sigma$ in above setup. (HINT: Use latex equations here, or mention in pdf. This [link](http://data-blog.udacity.com/posts/2016/10/latex-primer/) might be useful if you choose to write here).\n",
    "\n",
    "(b) Implement a function **sge()** that estimates the mean $\\mu$ and variance $\\sigma^{2}$ from the given data, using the skeleton code provided below. Note: You cannot use **numpy.cov** and **numpy.mean** or any other functions for calculating the mean and variance.\n",
    "\n",
    "(c) Implement a function **myplot1()** which takes as input a two-dimensional dataset $x$ (as described above); and draws, on the same plot, the following:\n",
    "1. A scatter plot of the original data $x$, \n",
    "2. Circles with center $\\mu$ and radius $r=k \\sigma$ for $k=1, 2, 3$ where $\\mathbf{\\mu}$ and $\\sigma^{2}$ denote the mean and variance estimated using **sge()**. \n",
    "3. Legend for each circle indicating the fraction of points (in the original dataset) that lie outside the circle boundary.\n",
    "\n",
    "(d) Load 'dataset0.txt' and run your code using only the first two features of the dataset. Submit the resulting plot as well as your implementation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ln f(\\vec{x}_i) = -1/2\\left( p \\ln(2\\pi) + p\\ln(\\sigma^2) + (\\vec{x}_i - \\mu)(\\sigma^2 I)^{-1}(\\vec{x}_i - \\mu)\\right)$\n",
    "\n",
    "Taking the derivative $\\ln L $ with respect to $\\mu$ yields:\n",
    "\n",
    "$ \\frac{\\partial ln(L)}{\\partial \\sigma^2} = \\Sigma_{i=0}^n \\left( -(\\sigma^2 I)^{-1}(\\vec{x}_i - \\mu) - (\\vec{x}_i - \\mu)(\\sigma^2 I)^{-1} \\right) = \\frac{-2}{\\sigma^2}\\Sigma_{i=0}^n (\\vec{x}_i - \\mu) $\n",
    "\n",
    "Then we solve $ \\frac{\\partial ln(L)}{\\partial \\mu} = 0$ for $\\mu$:\n",
    "\n",
    "$\\frac{-2}{\\sigma^2}\\Sigma_{i=0}^n (\\vec{x}_i - \\mu) = 0$\n",
    "\n",
    "$\\mu = \\frac{1}{n}\\Sigma_{i=0}^n \\vec{x}_i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sge(X):\n",
    "    \"\"\"\n",
    "    SGE Mean and variance estimator for spherical Gaussian distribution\n",
    "\n",
    "    X : Data matrix of size n x p where each row represents a p-dimensional data point\n",
    "    e.g. X = [ 2 1 ; 3 7 ; 4 5 ] is a dataset having 3 samples having two co−ordinates each.\n",
    "    mu : Estimated mean o f the dataset [mu_1 mu_2 . . . mu_p]\n",
    "    sigma : Estimated standard deviation of the dataset ( number )\n",
    "    \"\"\"\n",
    "    mu = ...\n",
    "    diffs = X - mu\n",
    "    variance  = 2/(n*p) \n",
    "    return mu,sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MAP estimation, 5 points]\n",
    "\n",
    "Consider dataset $x_1, \\ldots, x_n $ consisting of i.i.d. observations \n",
    "generated from a multivariate normal distribution $N(\\mu, \\sigma^2I)$, where $\\mu =\n",
    "[\\mu_{1},\\, \\mu_{2}]^{\\top} \\in \\mathbb{R}^2$, $I$ \n",
    "is the $2 \\times 2$ identity matrix, and $\\sigma^2$ is a scalar. We will now explore the Bayesian approach to estimation of $\\sigma^{2}$ *under the assumption that the mean $\\mu$ is known.*\n",
    "The probability distribution of a point $\\mathbf{x}=[x_{1},\\, x_{2}]^{\\top}$ is given by\n",
    "\n",
    "$$ P(X = x \\,|\\, \\sigma^{2}) =  \\frac{1}{ 2\\pi \\sigma^2}   \\exp\n",
    "\\left( -\\frac{ (x - \\mu)^{\\top}(x - \\mu) }{2\\sigma^{2}} \\right)\n",
    "~.$$\n",
    "\n",
    "We assume that $\\sigma^{2}$ has an **inverse-gamma** prior distribution\n",
    "given by\n",
    "$$ P(\\sigma^{2} = s | \\alpha, \\beta) =\n",
    "\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} s^{-\\alpha-1} \\exp\\left( -\n",
    "  \\frac{\\beta}{s}\\right)~. \\tag{1} $$\n",
    "  \n",
    "where $\\alpha$ and $\\beta$ are parameters and $\\Gamma(\\cdot)$ is the\n",
    "gamma function given by $\\Gamma(x) = \\int_{0}^{\\infty} t^{x-1} e^{-t}\n",
    "dt $.\n",
    "\n",
    "Assume that your dataset now consists of just the first two features of 'dataset0.txt'.\n",
    "\n",
    "(a) Choose $\\mu$ to be the empirical mean. Implement a function **myplot2()**, that on the same plot, shows the prior and posterior distributions for $\\sigma$ with parameters $\\alpha = 1 $ and $\\beta = 1$.  Generate a second plot with $\\alpha=10$ and $\\beta=1$. What do you observe?\n",
    "\n",
    "HINT:\n",
    "   * Calculate the posterior distribution using the data and the formula that you derived in the theoretical question **\"Posterior distributions\"**.\n",
    "   * You might want to check out the \"log-sum-exp trick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[526.37988827 419.97486034]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy' has no attribute 'special'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5cd068dd63de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_inv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brittathornblom1/venv1/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2753\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2757\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brittathornblom1/venv1/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2823\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2825\u001b[0;31m             \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0motypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m             \u001b[0;31m# Convert args to object arrays first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/brittathornblom1/venv1/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_get_ufunc_and_otypes\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2784\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2785\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m             \u001b[0;31m# Performance note: profiling indicates that -- for simple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5cd068dd63de>\u001b[0m in \u001b[0;36minverse_gamma_1\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minverse_gamma_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy' has no attribute 'special'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats.invgamma\n",
    "\n",
    "data = np.loadtxt('dataset0.txt')\n",
    "data = data[:,:2]\n",
    "\n",
    "mu = np.mean(data, axis=0)\n",
    "print(mu)\n",
    "\n",
    "\n",
    "def inverse_gamma_1(x):\n",
    "    a = 1\n",
    "    b = 1\n",
    "    return ((b**a)/scipy.special.gamma(a))*x**(-a-1)*np.exp(-b/x)\n",
    "\n",
    "def inverse_gamma_2(x):\n",
    "    a = 10\n",
    "    b = 1\n",
    "    return ((b**a)/scipy.special.gamma(a))*x**(-a-1)*np.exp(-b/x)\n",
    "\n",
    "vec_inv_1 = np.vectorize(inverse_gamma_1)\n",
    "vec_inv_2 = np.vectorize(inverse_gamma_2)\n",
    "\n",
    "\n",
    "sigma = np.linspace(0, 100, 100)\n",
    "prior = vec_inv_1(sigma)\n",
    "plt.plot(sigma, prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
